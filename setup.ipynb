{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf99\ufe0f Voice AI Training System - Setup\n",
    "## Environment Configuration & Validation\n",
    "\n",
    "This notebook will:\n",
    "1. \u2705 Detect and verify GPU availability\n",
    "2. \u2705 Install all required dependencies (Python 3.12+ compatible)\n",
    "3. \u2705 Mount Google Drive for persistent storage\n",
    "4. \u2705 Create project directory structure\n",
    "5. \u2705 Validate all imports and CUDA support\n",
    "6. \u2705 Display configuration summary\n",
    "\n",
    "**\u26a0\ufe0f Important:** Run all cells in order. This should complete in 3-5 minutes.\n",
    "\n",
    "**\ud83d\udcdd Note:** This system now uses Python 3.12 compatible TTS libraries (coqui-tts from Idiap Research Institute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: GPU Detection & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_info = !nvidia-smi\n",
    "print(\"\ud83d\udd0d GPU Detection:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Get GPU info\n",
    "    gpu_name = !nvidia-smi --query-gpu=name --format=csv,noheader\n",
    "    gpu_memory = !nvidia-smi --query-gpu=memory.total --format=csv,noheader\n",
    "    \n",
    "    if gpu_name and gpu_name[0]:\n",
    "        print(f\"\u2705 GPU Detected: {gpu_name[0]}\")\n",
    "        print(f\"\u2705 GPU Memory: {gpu_memory[0]}\")\n",
    "        \n",
    "        # Extract memory in GB\n",
    "        memory_str = gpu_memory[0].strip().split()[0]\n",
    "        gpu_memory_gb = int(memory_str) / 1024\n",
    "        print(f\"\u2705 GPU Memory (GB): {gpu_memory_gb:.1f} GB\")\n",
    "        \n",
    "        # Store for later use\n",
    "        os.environ['GPU_MEMORY_GB'] = str(gpu_memory_gb)\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f No GPU detected. This system requires GPU for training.\")\n",
    "        print(\"Please enable GPU in Runtime > Change runtime type > GPU\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Error detecting GPU: {e}\")\n",
    "    print(\"Please ensure GPU is enabled in Colab settings.\")\n",
    "    \n",
    "print(\"=\"*60)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udce6 Installing dependencies... This may take 2-3 minutes.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check Python version for compatibility\n",
    "import sys\n",
    "python_version = sys.version_info\n",
    "print(f\"\ud83d\udc0d Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "if python_version.major == 3 and python_version.minor >= 12:\n",
    "    print(\"\u2705 Python 3.12+ detected - using Python 3.12 compatible packages\")\n",
    "else:\n",
    "    print(f\"\u2139\ufe0f Python {python_version.major}.{python_version.minor} detected\")\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q torch>=2.1.0 torchaudio>=2.1.0 torchvision\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q datasets>=2.14.0\n",
    "!pip install -q librosa>=0.10.0\n",
    "!pip install -q soundfile>=0.12.0\n",
    "!pip install -q gradio>=4.0.0\n",
    "!pip install -q tqdm\n",
    "!pip install -q pandas\n",
    "!pip install -q numpy\n",
    "!pip install -q matplotlib\n",
    "!pip install -q scipy\n",
    "!pip install -q pydub\n",
    "!pip install -q pytorch-lightning>=2.0.0\n",
    "\n",
    "# Install TTS library (Python 3.12 Compatible)\n",
    "# Using Idiap's fork of Coqui TTS which supports Python 3.12+\n",
    "print(\"\ufffd\ufffd Installing TTS library (Python 3.12 compatible)...\")\n",
    "!pip install -q coqui-tts\n",
    "\n",
    "# Install bark (for lightweight option - may have limited Python 3.12 support)\n",
    "print(\"\ud83d\udce6 Installing Bark (alternative TTS)...\")\n",
    "!pip install -q git+https://github.com/suno-ai/bark.git\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\u2705 All dependencies installed successfully!\")\n",
    "print(\"\u2705 Primary TTS: coqui-tts (Python 3.12+ compatible - Idiap fork)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"\ud83d\udcc1 Mounting Google Drive...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"\u2705 Google Drive mounted successfully!\")\n",
    "    print(f\"\u2705 Drive path: /content/drive/MyDrive\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Error mounting drive: {e}\")\n",
    "    print(\"Please authorize Google Drive access when prompted.\")\n",
    "    \n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\ud83c\udfd7\ufe0f Creating directory structure...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create main directories\n",
    "base_dir = \"/content/voiceai\"\n",
    "drive_dir = \"/content/drive/MyDrive/voiceai\"\n",
    "\n",
    "directories = [\n",
    "    f\"{base_dir}/dataset\",\n",
    "    f\"{base_dir}/processed\",\n",
    "    f\"{base_dir}/checkpoints\",\n",
    "    f\"{base_dir}/outputs\",\n",
    "    f\"{base_dir}/logs\",\n",
    "    f\"{drive_dir}/checkpoints\",\n",
    "    f\"{drive_dir}/outputs\",\n",
    "    f\"{drive_dir}/logs\",\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"\u2705 Created: {directory}\")\n",
    "\n",
    "# Save paths for later use\n",
    "os.environ['BASE_DIR'] = base_dir\n",
    "os.environ['DRIVE_DIR'] = drive_dir\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\u2705 Directory structure created successfully!\")\n",
    "print(f\"\\n\ud83d\udcc2 Local workspace: {base_dir}\")\n",
    "print(f\"\ud83d\udcc2 Persistent storage: {drive_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validate Imports & CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"\ud83d\udd0d Validating imports and CUDA support...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import_status = {}\n",
    "\n",
    "# Test critical imports\n",
    "try:\n",
    "    import torch\n",
    "    import_status['torch'] = f\"\u2705 v{torch.__version__}\"\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    cuda_version = torch.version.cuda if cuda_available else \"N/A\"\n",
    "    import_status['CUDA'] = f\"\u2705 Available (v{cuda_version})\" if cuda_available else \"\u274c Not Available\"\n",
    "    \n",
    "    if cuda_available:\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        import_status['GPU Device'] = f\"\u2705 {device_name}\"\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        import_status['GPU Memory'] = f\"\u2705 {gpu_memory:.1f} GB\"\n",
    "except Exception as e:\n",
    "    import_status['torch'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    import_status['torchaudio'] = f\"\u2705 v{torchaudio.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['torchaudio'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    import_status['transformers'] = f\"\u2705 v{transformers.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['transformers'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import TTS\n",
    "    import_status['coqui-tts (Python 3.12+)'] = f\"\u2705 Installed\"\n",
    "except Exception as e:\n",
    "    import_status['coqui-tts (Python 3.12+)'] = f\"\u274c Error: {e}\"\n",
    "    import_status['TTS (Coqui)'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "    import_status['librosa'] = f\"\u2705 v{librosa.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['librosa'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import gradio\n",
    "    import_status['gradio'] = f\"\u2705 v{gradio.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['gradio'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "    import_status['pytorch_lightning'] = f\"\u2705 v{pl.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['pytorch_lightning'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "    import_status['numpy'] = f\"\u2705 v{numpy.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['numpy'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import pandas\n",
    "    import_status['pandas'] = f\"\u2705 v{pandas.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['pandas'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "    import_status['matplotlib'] = f\"\u2705 v{matplotlib.__version__}\"\n",
    "except Exception as e:\n",
    "    import_status['matplotlib'] = f\"\u274c Error: {e}\"\n",
    "\n",
    "# Display results\n",
    "for package, status in import_status.items():\n",
    "    print(f\"{package:20} : {status}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if all critical packages are available\n",
    "critical_packages = ['torch', 'CUDA', 'transformers', 'coqui-tts (Python 3.12+)', 'librosa', 'gradio']\n",
    "all_good = all('\u2705' in import_status.get(pkg, '') for pkg in critical_packages)\n",
    "\n",
    "if all_good:\n",
    "    print(\"\u2705 All critical imports validated successfully!\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Some packages failed to import. Please check errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udfaf VOICE AI SYSTEM CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "print(\"\\n\ud83d\udcca System Information:\")\n",
    "print(f\"  \u2022 Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"  \u2022 PyTorch Version: {torch.__version__}\")\n",
    "print(f\"  \u2022 CUDA Available: {'Yes \u2705' if torch.cuda.is_available() else 'No \u274c'}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  \u2022 CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  \u2022 GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"  \u2022 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Recommend model based on GPU memory\n",
    "    print(f\"\\n\ud83e\udd16 Recommended Model:\")\n",
    "    if gpu_memory >= 16:\n",
    "        print(f\"  \u2022 XTTS v2 (Coqui TTS) - High quality, multi-speaker\")\n",
    "        print(f\"  \u2022 GPU Memory sufficient for full training\")\n",
    "        recommended_model = \"XTTS\"\n",
    "    else:\n",
    "        print(f\"  \u2022 Bark (Suno) - Lightweight, expressive\")\n",
    "        print(f\"  \u2022 GPU Memory optimized for efficient training\")\n",
    "        recommended_model = \"Bark\"\n",
    "    \n",
    "    os.environ['RECOMMENDED_MODEL'] = recommended_model\n",
    "\n",
    "# Paths\n",
    "print(f\"\\n\ud83d\udcc2 Directory Structure:\")\n",
    "print(f\"  \u2022 Local workspace: /content/voiceai\")\n",
    "print(f\"  \u2022 Persistent storage: /content/drive/MyDrive/voiceai\")\n",
    "print(f\"  \u2022 Dataset: /content/voiceai/dataset\")\n",
    "print(f\"  \u2022 Checkpoints: /content/drive/MyDrive/voiceai/checkpoints\")\n",
    "print(f\"  \u2022 Outputs: /content/drive/MyDrive/voiceai/outputs\")\n",
    "\n",
    "# Model options\n",
    "print(f\"\\n\ud83c\udf99\ufe0f Available TTS Models:\")\n",
    "  \u2022 Using coqui-tts (Idiap fork) - Python 3.12+ compatible\n",
    "print(f\"  1. XTTS v2 (Coqui) - Best for: High-quality, emotional speech\")\n",
    "print(f\"  2. Bark (Suno) - Best for: Lightweight, fast inference\")\n",
    "\n",
    "# Language support\n",
    "print(f\"\\n\ud83c\udf0d Language Support:\")\n",
    "print(f\"  \u2022 Default: English (LJSpeech dataset)\")\n",
    "print(f\"  \u2022 Custom: Upload multilingual datasets (Hindi, Spanish, etc.)\")\n",
    "\n",
    "# Training config\n",
    "print(f\"\\n\u2699\ufe0f Default Training Configuration:\")\n",
    "print(f\"  \u2022 Learning Rate: 1e-4\")\n",
    "print(f\"  \u2022 Batch Size: Auto-adjusted based on GPU\")\n",
    "print(f\"  \u2022 Epochs: 5 (configurable)\")\n",
    "print(f\"  \u2022 Optimizer: AdamW\")\n",
    "print(f\"  \u2022 Mixed Precision: Enabled (FP16)\")\n",
    "print(f\"  \u2022 Gradient Clipping: 1.0\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2705 Setup complete \u2014 environment ready\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\ud83d\ude80 Next Steps:\")\n",
    "print(\"  1. Open train_or_finetune.ipynb to prepare data and train model\")\n",
    "print(\"  2. Open demo.ipynb to run inference and demo\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "setup.ipynb",
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}