import torch
import os
import sys

print("="*60)
print("ğŸ¯ VOICE AI SYSTEM CONFIGURATION SUMMARY")
print("="*60)

# System info
print("\nğŸ“Š System Information:")
print(f"  â€¢ Python Version: {sys.version.split()[0]}")
print(f"  â€¢ PyTorch Version: {torch.__version__}")
print(f"  â€¢ CUDA Available: {'Yes âœ…' if torch.cuda.is_available() else 'No âŒ'}")

if torch.cuda.is_available():
    print(f"  â€¢ CUDA Version: {torch.version.cuda}")
    print(f"  â€¢ GPU Device: {torch.cuda.get_device_name(0)}")
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"  â€¢ GPU Memory: {gpu_memory:.1f} GB")
    
    # Recommend model based on GPU memory
    print(f"\nğŸ¤– Recommended Model:")
    if gpu_memory >= 16:
        print(f"  â€¢ XTTS v2 (Coqui TTS) - High quality, multi-speaker")
        print(f"  â€¢ GPU Memory sufficient for full training")
        recommended_model = "XTTS"
    else:
        print(f"  â€¢ Bark (Suno) - Lightweight, expressive")
        print(f"  â€¢ GPU Memory optimized for efficient training")
        recommended_model = "Bark"
    
    os.environ['RECOMMENDED_MODEL'] = recommended_model

# Paths
print(f"\nğŸ“‚ Directory Structure:")
print(f"  â€¢ Local workspace: /content/voiceai")
print(f"  â€¢ Persistent storage: /content/drive/MyDrive/voiceai")
print(f"  â€¢ Dataset: /content/voiceai/dataset")
print(f"  â€¢ Checkpoints: /content/drive/MyDrive/voiceai/checkpoints")
print(f"  â€¢ Outputs: /content/drive/MyDrive/voiceai/outputs")

# Model options
print(f"\nğŸ™ï¸ Available TTS Models:")
print(f"  1. XTTS v2 (Coqui) - Best for: High-quality, emotional speech")
print(f"  2. Bark (Suno) - Best for: Lightweight, fast inference")

# Language support
print(f"\nğŸŒ Language Support:")
print(f"  â€¢ Default: English (LJSpeech dataset)")
print(f"  â€¢ Custom: Upload multilingual datasets (Hindi, Spanish, etc.)")

# Training config
print(f"\nâš™ï¸ Default Training Configuration:")
print(f"  â€¢ Learning Rate: 1e-4")
print(f"  â€¢ Batch Size: Auto-adjusted based on GPU")
print(f"  â€¢ Epochs: 5 (configurable)")
print(f"  â€¢ Optimizer: AdamW")
print(f"  â€¢ Mixed Precision: Enabled (FP16)")
print(f"  â€¢ Gradient Clipping: 1.0")

print("\n" + "="*60)
print("âœ… Setup complete â€” environment ready")
print("="*60)
print("\nğŸš€ Next Steps:")
print("  1. Open train_or_finetune.ipynb to prepare data and train model")
print("  2. Open demo.ipynb to run inference and demo")
print("="*60)## Step 6: Configuration Summaryimport sys
print("ğŸ” Validating imports and CUDA support...")
print("="*60)

import_status = {}

# Test critical imports
try:
    import torch
    import_status['torch'] = f"âœ… v{torch.__version__}"
    cuda_available = torch.cuda.is_available()
    cuda_version = torch.version.cuda if cuda_available else "N/A"
    import_status['CUDA'] = f"âœ… Available (v{cuda_version})" if cuda_available else "âŒ Not Available"
    
    if cuda_available:
        device_name = torch.cuda.get_device_name(0)
        import_status['GPU Device'] = f"âœ… {device_name}"
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        import_status['GPU Memory'] = f"âœ… {gpu_memory:.1f} GB"
except Exception as e:
    import_status['torch'] = f"âŒ Error: {e}"

try:
    import torchaudio
    import_status['torchaudio'] = f"âœ… v{torchaudio.__version__}"
except Exception as e:
    import_status['torchaudio'] = f"âŒ Error: {e}"

try:
    import transformers
    import_status['transformers'] = f"âœ… v{transformers.__version__}"
except Exception as e:
    import_status['transformers'] = f"âŒ Error: {e}"

try:
    import TTS
    import_status['TTS (Coqui)'] = f"âœ… Installed"
except Exception as e:
    import_status['TTS (Coqui)'] = f"âŒ Error: {e}"

try:
    import librosa
    import_status['librosa'] = f"âœ… v{librosa.__version__}"
except Exception as e:
    import_status['librosa'] = f"âŒ Error: {e}"

try:
    import gradio
    import_status['gradio'] = f"âœ… v{gradio.__version__}"
except Exception as e:
    import_status['gradio'] = f"âŒ Error: {e}"

try:
    import pytorch_lightning as pl
    import_status['pytorch_lightning'] = f"âœ… v{pl.__version__}"
except Exception as e:
    import_status['pytorch_lightning'] = f"âŒ Error: {e}"

try:
    import numpy
    import_status['numpy'] = f"âœ… v{numpy.__version__}"
except Exception as e:
    import_status['numpy'] = f"âŒ Error: {e}"

try:
    import pandas
    import_status['pandas'] = f"âœ… v{pandas.__version__}"
except Exception as e:
    import_status['pandas'] = f"âŒ Error: {e}"

try:
    import matplotlib
    import_status['matplotlib'] = f"âœ… v{matplotlib.__version__}"
except Exception as e:
    import_status['matplotlib'] = f"âŒ Error: {e}"

# Display results
for package, status in import_status.items():
    print(f"{package:20} : {status}")

print("="*60)

# Check if all critical packages are available
critical_packages = ['torch', 'CUDA', 'transformers', 'TTS (Coqui)', 'librosa', 'gradio']
all_good = all('âœ…' in import_status.get(pkg, '') for pkg in critical_packages)

if all_good:
    print("âœ… All critical imports validated successfully!")
else:
    print("âš ï¸ Some packages failed to import. Please check errors above.")## Step 5: Validate Imports & CUDAimport os

print("ğŸ—ï¸ Creating directory structure...")
print("="*60)

# Create main directories
base_dir = "/content/voiceai"
drive_dir = "/content/drive/MyDrive/voiceai"

directories = [
    f"{base_dir}/dataset",
    f"{base_dir}/processed",
    f"{base_dir}/checkpoints",
    f"{base_dir}/outputs",
    f"{base_dir}/logs",
    f"{drive_dir}/checkpoints",
    f"{drive_dir}/outputs",
    f"{drive_dir}/logs",
]

for directory in directories:
    os.makedirs(directory, exist_ok=True)
    print(f"âœ… Created: {directory}")

# Save paths for later use
os.environ['BASE_DIR'] = base_dir
os.environ['DRIVE_DIR'] = drive_dir

print("="*60)
print("âœ… Directory structure created successfully!")
print(f"\nğŸ“‚ Local workspace: {base_dir}")
print(f"ğŸ“‚ Persistent storage: {drive_dir}")## Step 4: Create Directory Structurefrom google.colab import drive
import os

print("ğŸ“ Mounting Google Drive...")
print("="*60)

try:
    drive.mount('/content/drive', force_remount=False)
    print("âœ… Google Drive mounted successfully!")
    print(f"âœ… Drive path: /content/drive/MyDrive")
except Exception as e:
    print(f"âš ï¸ Error mounting drive: {e}")
    print("Please authorize Google Drive access when prompted.")
    
print("="*60)## Step 3: Mount Google Driveprint("ğŸ“¦ Installing dependencies... This may take 2-3 minutes.")
print("="*60)

# Install core dependencies
!pip install -q torch>=2.1.0 torchaudio>=2.1.0 torchvision
!pip install -q transformers>=4.35.0
!pip install -q accelerate>=0.24.0
!pip install -q datasets>=2.14.0
!pip install -q librosa>=0.10.0
!pip install -q soundfile>=0.12.0
!pip install -q gradio>=4.0.0
!pip install -q tqdm
!pip install -q pandas
!pip install -q numpy
!pip install -q matplotlib
!pip install -q scipy
!pip install -q pydub
!pip install -q pytorch-lightning>=2.0.0

# Install TTS library (Coqui)
!pip install -q TTS

# Install bark (for lightweight option)
!pip install -q git+https://github.com/suno-ai/bark.git

print("="*60)
print("âœ… All dependencies installed successfully!")## Step 2: Install Dependenciesimport subprocess
import sys
import os

# Check GPU availability
gpu_info = !nvidia-smi
print("ğŸ” GPU Detection:")
print("="*60)

try:
    # Get GPU info
    gpu_name = !nvidia-smi --query-gpu=name --format=csv,noheader
    gpu_memory = !nvidia-smi --query-gpu=memory.total --format=csv,noheader
    
    if gpu_name and gpu_name[0]:
        print(f"âœ… GPU Detected: {gpu_name[0]}")
        print(f"âœ… GPU Memory: {gpu_memory[0]}")
        
        # Extract memory in GB
        memory_str = gpu_memory[0].strip().split()[0]
        gpu_memory_gb = int(memory_str) / 1024
        print(f"âœ… GPU Memory (GB): {gpu_memory_gb:.1f} GB")
        
        # Store for later use
        os.environ['GPU_MEMORY_GB'] = str(gpu_memory_gb)
    else:
        print("âš ï¸ No GPU detected. This system requires GPU for training.")
        print("Please enable GPU in Runtime > Change runtime type > GPU")
        
except Exception as e:
    print(f"âš ï¸ Error detecting GPU: {e}")
    print("Please ensure GPU is enabled in Colab settings.")
    
print("="*60)
print(f"Python Version: {sys.version}")
print("="*60)## Step 1: GPU Detection & Verification# ğŸ™ï¸ Voice AI Training System - Setup
## Environment Configuration & Validation

This notebook will:
1. âœ… Detect and verify GPU availability
2. âœ… Install all required dependencies
3. âœ… Mount Google Drive for persistent storage
4. âœ… Create project directory structure
5. âœ… Validate all imports and CUDA support
6. âœ… Display configuration summary

**âš ï¸ Important:** Run all cells in order. This should complete in 3-5 minutes.